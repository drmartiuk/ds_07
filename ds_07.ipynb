{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7514447d",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593d100",
   "metadata": {},
   "source": [
    "#### Описание проекта\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "Постройте модель со значением метрики качества F1 не меньше 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c3c93",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428602cb",
   "metadata": {},
   "source": [
    "### Ипорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae37ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python: Error while finding module specification for 'textblob.download_corpora' (ModuleNotFoundError: No module named 'textblob')\r\n"
     ]
    }
   ],
   "source": [
    "#pip install pymystem3\n",
    "#!pip install --upgrade pip\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29304049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import time\n",
    "from ipywidgets import FloatProgress\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d111676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments=pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e9ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(dataset,name):\n",
    "    display(f'Анализ датафрейма {name}')\n",
    "    display(dataset.head(5))\n",
    "    display(f'Количество пустых ячеек:')\n",
    "    display(dataset.isnull().sum())\n",
    "    display(dataset.info())\n",
    "    display(f'Количество явных дубликатов в датасете:{dataset.duplicated().sum()}' )\n",
    "    dataset=dataset.drop_duplicates()\n",
    "    display(f'Количество явных дубликатов в датасете после удаления:{dataset.duplicated().sum()}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f567a130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Анализ датафрейма toxic_comments'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Количество пустых ячеек:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "text          0\n",
       "toxic         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Количество явных дубликатов в датасете:0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Количество явных дубликатов в датасете после удаления:0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_info(toxic_comments, 'toxic_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc85153",
   "metadata": {},
   "source": [
    "###  Выводы\n",
    "Текст необходимо лемматизировать и убрать символы. Пропуски и дубликаты отсутствуют"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b1ea2",
   "metadata": {},
   "source": [
    "Расчетное время лемматизации у меня было в районе 30 часов. Может что-то не так делал, но по образу и подобию из лекций взял материал. На хабре нашел метод, который переработал немного и применил в ячейке ниже. Лемматизация не сработала почему-то(  https://habr.com/ru/articles/503420/ Нашел еще функцию и быстрее ее ничего не смог сделать. Взял за основу в работе "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539c18fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee9dfd5cb4a4d8493044e5995111727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_comments=toxic_comments.drop('Unnamed: 0',axis=1)\n",
    "toxic_comments['text'] = toxic_comments['text'].str.lower()\n",
    "\n",
    "def clear_text(text):\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "toxic_comments['text_edited'] = toxic_comments['text'].progress_apply(lambda x: clear_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947c9e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f4e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2367e6bb388241b58844d60af357e3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "\n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "\n",
    "toxic_comments['lemm'] =toxic_comments['text_edited'].progress_apply(lemmatize_sent)\n",
    "toxic_comments['lemm']=toxic_comments['lemm'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497ca84",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9400b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "TEST_SIZE=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed399a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = toxic_comments['toxic']\n",
    "X = toxic_comments['lemm']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=TEST_SIZE,random_state=RANDOM_STATE)\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=list(stopwords))\n",
    "X_train = count_tf_idf.fit_transform(X_train)\n",
    "X_test = count_tf_idf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547f2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model_LR = LogisticRegression(random_state=RANDOM_STATE)\n",
    "params = [{'solver':['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'C':[0.1, 1, 10]}]\n",
    "clf = GridSearchCV(model_LR, params, scoring='f1',cv=2)\n",
    "clf.fit(X_train, y_train)\n",
    "LR_best_params = clf.best_params_\n",
    "model_LR.set_params(**LR_best_params)\n",
    "model_LR.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "education_time_LR = end_time - start_time\n",
    "f1_LR = clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e82ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "param_search_lgb = {'max_depth': [4, 6, 10, 12],\n",
    "                    'num_leaves': [30, 40, 50, 60]}\n",
    "\n",
    "model_LGBMC = LGBMClassifier(n_estimators=100,learning_rate=0.3,class_weight='balanced',random_state=RANDOM_STATE,force_col_wise=True)\n",
    "\n",
    "clf = GridSearchCV(model_LGBMC, param_grid=param_search_lgb, scoring='f1',cv=2,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "LGBMC_best_params = clf.best_params_\n",
    "model_LGBMC.set_params(**LGBMC_best_params)\n",
    "model_LGBMC.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "education_time_LGBMC = end_time - start_time\n",
    "f1_LGBMC = clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa6ae926",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "param_search_SGDC =  {'alpha': [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "model_SGDC = SGDClassifier(max_iter=100,tol=0.001,class_weight='balanced',random_state=RANDOM_STATE)\n",
    "\n",
    "clf = GridSearchCV(model_SGDC, param_grid=param_search_SGDC, scoring='f1',cv=2,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "SGDC_best_params = clf.best_params_\n",
    "model_SGDC.set_params(**SGDC_best_params)\n",
    "model_SGDC.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "education_time_SGDC = end_time - start_time\n",
    "f1_SGDC = clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229c3a2",
   "metadata": {},
   "source": [
    "###  Выводы\n",
    "Обучены разные модели: LogisticRegression,LGBMClassifier,SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892b2c1",
   "metadata": {},
   "source": [
    "## Анализ моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90fb672d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>education_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.750696</td>\n",
       "      <td>399.720850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.744974</td>\n",
       "      <td>1987.077929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.730765</td>\n",
       "      <td>4.542535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          F1  education_time\n",
       "LogisticRegression  0.750696      399.720850\n",
       "LGBMClassifier      0.744974     1987.077929\n",
       "SGDClassifier       0.730765        4.542535"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = ['LogisticRegression',\n",
    "         'LGBMClassifier',\n",
    "        'SGDClassifier']\n",
    "data = {'F1':[f1_LR,f1_LGBMC,f1_SGDC],'education_time':[education_time_LR,education_time_LGBMC,education_time_SGDC]}\n",
    "display(pd.DataFrame(data=data, index=index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e063829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prediction_time_LogisticRegression_test '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0035238265991210938"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'f1_LogisticRegression_test '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7731741573033708"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "target_predict = model_LR.predict(X_test)\n",
    "end_time = time.time()\n",
    "prediction_time_LR_test = end_time - start_time\n",
    "\n",
    "f1_LR_test = f1_score( y_test,target_predict)\n",
    "\n",
    "display('prediction_time_LogisticRegression_test ',prediction_time_LR_test)\n",
    "display('f1_LogisticRegression_test ',f1_LR_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf8c94",
   "metadata": {},
   "source": [
    "###  Выводы\n",
    "По результатам анализа моделей можно сделать вывод, что величина f1 самая высокая (0,75) у логистической регрессии. \n",
    "Стоит остановить выбор на данной модели. f1 0,77"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 792,
    "start_time": "2024-07-30T19:32:38.765Z"
   },
   {
    "duration": 1570,
    "start_time": "2024-07-30T19:32:39.559Z"
   },
   {
    "duration": 2419,
    "start_time": "2024-07-30T19:32:41.131Z"
   },
   {
    "duration": 4,
    "start_time": "2024-07-30T19:32:43.551Z"
   },
   {
    "duration": 666,
    "start_time": "2024-07-30T19:32:43.557Z"
   },
   {
    "duration": 903,
    "start_time": "2024-07-31T09:11:58.857Z"
   },
   {
    "duration": 1721,
    "start_time": "2024-07-31T09:11:59.763Z"
   },
   {
    "duration": 2650,
    "start_time": "2024-07-31T09:12:01.486Z"
   },
   {
    "duration": 4,
    "start_time": "2024-07-31T09:12:04.138Z"
   },
   {
    "duration": 735,
    "start_time": "2024-07-31T09:12:04.143Z"
   },
   {
    "duration": 5456,
    "start_time": "2024-07-31T09:12:04.880Z"
   },
   {
    "duration": 440,
    "start_time": "2024-07-31T09:12:10.338Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-31T09:12:10.781Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-31T09:12:10.782Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-31T09:12:10.784Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-31T09:12:10.785Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-31T09:12:10.787Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-31T09:12:10.789Z"
   },
   {
    "duration": 0,
    "start_time": "2024-07-31T09:12:10.790Z"
   },
   {
    "duration": 307,
    "start_time": "2024-07-31T09:13:22.123Z"
   },
   {
    "duration": 900,
    "start_time": "2024-07-31T09:13:40.952Z"
   },
   {
    "duration": 1717,
    "start_time": "2024-07-31T09:13:41.854Z"
   },
   {
    "duration": 2292,
    "start_time": "2024-07-31T09:13:43.573Z"
   },
   {
    "duration": 4,
    "start_time": "2024-07-31T09:13:45.867Z"
   },
   {
    "duration": 730,
    "start_time": "2024-07-31T09:13:45.873Z"
   },
   {
    "duration": 5422,
    "start_time": "2024-07-31T09:13:46.606Z"
   },
   {
    "duration": 10,
    "start_time": "2024-07-31T09:13:52.030Z"
   },
   {
    "duration": 579893,
    "start_time": "2024-07-31T09:13:52.041Z"
   },
   {
    "duration": 3,
    "start_time": "2024-07-31T09:23:31.935Z"
   },
   {
    "duration": 8713,
    "start_time": "2024-07-31T09:23:31.940Z"
   },
   {
    "duration": 399725,
    "start_time": "2024-07-31T09:23:40.654Z"
   },
   {
    "duration": 1987082,
    "start_time": "2024-07-31T09:30:20.381Z"
   },
   {
    "duration": 4546,
    "start_time": "2024-07-31T10:03:27.465Z"
   },
   {
    "duration": 9,
    "start_time": "2024-07-31T10:03:32.014Z"
   },
   {
    "duration": 24,
    "start_time": "2024-07-31T10:03:32.024Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
